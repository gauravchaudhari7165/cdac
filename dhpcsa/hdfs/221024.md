# HDFS Commands and User Management

## HDFS Operations

1. **Start HDFS Services**
    ```sh
    start-all.sh
    ```

2. **List Root Directory**
    ```sh
    hdfs dfs -ls /
    ```

3. **Create Directory `/acts`**
    ```sh
    hdfs dfs -mkdir /acts
    hdfs dfs -ls /
    ```

4. **Create and Copy File to HDFS**
    ```sh
    nano hdfs-demo.txt
    hdfs dfs -copyFromLocal hdfs-demo.txt /acts/
    ```

5. **Local Directory Operations**
    ```sh
    mkdir test
    cd test
    hdfs dfs -copyToLocal /acts/hdfs-demo.txt
    ls
    ```

6. **Remove and Retrieve File**
    ```sh
    rm -rf hdfs-demo.txt
    ls
    hdfs dfs -get /acts/hdfs-demo.txt
    ls
    ```

7. **List and Manage HDFS Directories**
    ```sh
    hdfs dfs -ls /
    hdfs dfs -ls /demo
    hdfs dfs -rm /demo/multi-node.txt
    hdfs dfs -mkdir /demo
    hdfs dfs -cp /acts/hdfs-demo.txt /demo/
    ```

## User Management

1. **Add and Switch User**
    ```sh
    sudo adduser user2
    su - user2
    ```

2. **Create and List File for User2**
    ```sh
    nano user2.txt
    # content for this file->> this is user2
    hdfs dfs -ls /
    ```

3. **HDFS Operations as User2**
    ```sh
    cd /usr/local/hadoop/bin
    ./hdfs dfs -ls /
    ./hdfs dfs -mkdir /user2
    exit
    hdfs dfs -mkdir /user2-data
    hdfs dfs -ls /
    hdfs dfs -chown user2 /user2-data
    hdfs dfs -ls /
    su - user2
    cd /usr/local/hadoop/bin/
    cd
    export PATH=$PATH:/usr/local/hadoop/bin
    hdfs dfs -ls /
    nano user2.txt
    hdfs dfs -put user2.txt /user2-data
    hdfs dfs -ls /user2-data
    exit
    ```

4. **Group and User Management**
    ```sh
    sudo groupadd hpcsa
    sudo adduser hpc1
    sudo adduser --ingroup hpcsa hpc2
    sudo usermod -G hpcsa hpc1
    ```

5. **HDFS Directory and Permissions**
    ```sh
    hdfs dfs -mkdir /hpc-project
    hdfs dfs -ls /
    hdfs dfs -chgrp hpcsa /hpc-project
    hdfs dfs -ls /
    hdfs dfs -chmod 775 /hpc-project
    hdfs dfs -ls /
    ```

6. **User Operations in HDFS**
    ```sh
    su - hpc1
    nano hpc1.txt
    hdfs dfs -put hpc1.txt /hpc-project
    export PATH=$PATH:/usr/local/hadoop/bin
    hdfs dfs -put hpc1.txt /hpc-project
    exit

    su - hpc2
    export PATH=$PATH:/usr/local/hadoop/bin
    nano hpc2.txt
    hdfs dfs -put hpc2.txt /hpc-project
    exit
    hdfs dfs -ls /hpc-project
    su - hpc1
    hdfs dfs -cat /hpc-project/hpc1.txt
    ```

7. **System-wide PATH Configuration**
    ```sh
    sudo nano /etc/bashrc
    export PATH=$PATH:/usr/local/Hadoop

    sudo nano /etc/bash.bashrc
    export PATH=$PATH:/usr/local/hadoop/bin
    exit
    su - hpc1
    hdfs dfs -ls /
    nano file2.txt
    hdfs dfs -cat /hpc-project/hpc1.txt
    hdfs dfs -appendToFile file2.txt /hpc-project/hpc1.txt
    hdfs dfs -cat /hpc-project/hpc1.txt
    ```

# 231024

    
        Start the Hadoop cluster 
        create a directory survey on HDFS
        Make sure users s1 and s2 are able 
        to upload files to survey directory 
        Make sure they are not able to delete each 
        Others files Also make sure other users
        are not able to access data from survey directory 
        Try with user tom or jerry






        



## Survey Directory Setup
1. **Start Hadoop Cluster**
    ```sh
    start-all.sh
    ```

2. **Create Survey Directory on HDFS**
    ```sh
    hdfs dfs -mkdir /survey
    hdfs dfs -ls /
    ```
3. **Create a New Group**
            ```sh
            sudo groupadd survey
            ```

4. **Create Users s1 and s2**
    ```sh
    sudo adduser --ingroup survey s1
    sudo adduser --ingroup survey s2
    ```

5. **Set Permissions for Survey Directory**
     ```sh
    hdfs dfs -chgrp survey /survey
    hdfs dfs -chmod 770 /survey
    ```

6. **Allow s1 and s2 to Upload Files**
    ```sh
    hdfs dfs -t /survey
    ```

7. **Test Upload and Access Restrictions**
    ```sh
    su - s1
    nano s1-file.txt
    hdfs dfs -put s1-file.txt /survey
    exit

    su - s2
    nano s2-file.txt
    hdfs dfs -put s2-file.txt /survey
    exit

    su - s2
    hdfs dfs -rm -rf /survey/s1-file.txt


    su - tom
    hdfs dfs -ls /survey
    exit

    su - jerry
    hdfs dfs -ls /survey
    exit
    ```

7. **Verify Permissions**
    ```sh
    su - s1
    hdfs dfs -rm /survey/s2-file.txt
    exit

    su - s2
    hdfs dfs -rm /survey/s1-file.txt
    exit
    ```

8. **Ensure Other Users Cannot Access Survey Directory**
    ```sh
    su - tom
    hdfs dfs -ls /survey
    exit

    su - jerry
    hdfs dfs -ls /survey
    exit
    ```

        Create a another directory s2-data on hdfs 
        Make Sure only s2 user can store data in this dir 
        login as s2 user 
        copy file in survey directory to s2-data 
        directory





## s2-data Directory Setup

1. **Create s2-data Directory on HDFS**
    ```sh
    hdfs dfs -mkdir /s2-data
    hdfs dfs -ls /
    ```

2. **Set Permissions for s2-data Directory**
    ```sh
    hdfs dfs -chown s2:s2 /s2-data
    hdfs dfs -chmod 700 /s2-data
    ```

3. **Login as s2 User and Copy File**
    ```sh
    su - s2
    hdfs dfs -cp /survey/s2-file.txt /s2-data/
    hdfs dfs -ls /s2-data
    exit
    ```

4. **Verify Permissions**
    ```sh
    su - s1
    hdfs dfs -put s1-file.txt /s2-data
    exit

    su - tom
    hdfs dfs -ls /s2-data
    exit

    su - jerry
    hdfs dfs -ls /s2-data
    exit
    ```







# Acl demo
hdfs dfs -mkdir /acl-demo
nano acl.txt
hdfs dfs -put acl.txt /acl-demo/
hdfs dfs -getfacl /acl-demo/acl.txt
sudo adduser s1
sudo adduser s2
hdfs dfs -setfacl -m user:s1:rwx /acl-demo/acl.txt
cd /usr/local/hadoop/etc/hadoop
nano hdfs-site.xml
<property>
<name>dfs.namenode.acls.enabled</name>
<value>true</value>
</property>
hdfs dfsadmin -refreshNodes
hadoop-daemon.sh stop namenode
hdfs --daemon stop namenode
hdfs --daemon start namenode
hdfs dfs -setfacl -m user:s1:rwx /acl-demo/acl.txt
hdfs dfs -getfacl /acl-demo/acl.txt
hdfs dfs -setfacl -m user:s2:r-x /acl-demo/acl.txt



## for node 3 
hdfs dfsadmin -report(show cluster information) 
cat /etc/hosts
then, go to node 1
nano /etc/hosts 
add entry of node 3
then, go to node 2
nano /etc/hosts 
add entry of node 3


sudo nano /etc/hosts
scp .bashrc hduser@node3:/home/hduser/ 
scp /etc/hosts hduser@node3:/home/hduser/

ssh -t hduser@node3 "sudo mkdir /usr/local/hadoop" 
ssh -t hduser@node3 "sudo chown hduser:hduser /usr/local/hadoop" 
ssh -t hduser@node3 "sudo chmod 755 /usr/local/hadoop"




on manager
cd /usr/local/hadoop/etc/hadoop
nano workers
add node 3 entry 


su node3 
ping manager 
echo $HADOOP_HOME 
echo $JAVA_HOME 
jps  
hadoop-daemon.sh stop datanode
hadoop-daemon.sh start datanode

nano acts.txt 
hdfs dfs -mkdir /test2 
hdfs dfs -put acts.txt /test2
hdfs dfs -ls /test2
hdfs fsck / -files -blocks -locations | less
nano repl-demo.txt
hdfs dfs -D dfs.replication=3 -put repl-demo.txt/ test2
hdfs fsck / -blocks -files -locations
nano repl-fail.txt
hdfs dfs -D dfs.replications=4 -put repl-fail.txt /test2
hdfs fsck / -blocks -files -locations 
hdfs dfs -rm -r -f /test2
cd /usr/local/hadoop/etc/hadoop
/usr/local/hadoop/rtc/hadoop$ hdfs --aemon start namenode
/user/local/hadoop/etc/hadoop$
/usr/local/hadoop//logs$ cd ../etc/hadoop
/usr/local/hadoop/etc/hadoop$ nano dfs.hosts.exclude
/usr/local/hadoop/etc/hadoop$ hdfs dfsadmin -refresh refreshNodes


dfs.hots.exclude</name>
/usr/loca/hadoop/etc/hadoop/dfs.hosts.exclude</value>


hdfs --daemon start datanode
yarn --daemon start nodemanager 
exit 
hdfs dfsadmin -report




what is the default block size of hadoop version 3.2
128 MB